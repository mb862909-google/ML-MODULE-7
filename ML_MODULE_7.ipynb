{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**# Question What is a Decision Tree, and how does it work?**\n",
        "\n",
        "Answer:\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MzrbM8bGmhjk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A **Decision Tree** is a popular supervised machine learning algorithm used for **classification** and **regression** tasks. It mimics human decision-making by splitting data into branches based on feature values to arrive at a decision (output). Think of it like a flowchart: each internal node asks a question about a feature, branches represent possible answers, and leaves represent the final prediction.\n",
        "\n",
        "Here’s a detailed explanation of how it works:\n",
        "\n",
        "---\n",
        "\n",
        "### **Structure of a Decision Tree**\n",
        "\n",
        "1. **Root Node**:\n",
        "\n",
        "   * The topmost node in the tree.\n",
        "   * Represents the **feature that best splits the data**.\n",
        "\n",
        "2. **Internal Nodes**:\n",
        "\n",
        "   * Nodes that perform tests on features.\n",
        "   * Each node splits the dataset based on a condition (e.g., `Age > 30?`).\n",
        "\n",
        "3. **Branches**:\n",
        "\n",
        "   * Outcome of a test, leading to the next node or leaf.\n",
        "\n",
        "4. **Leaf Nodes (Terminal Nodes)**:\n",
        "\n",
        "   * Final nodes that **give the prediction**.\n",
        "   * For classification: the predicted class.\n",
        "   * For regression: the predicted numeric value.\n",
        "\n",
        "---\n",
        "\n",
        "### **How It Works**\n",
        "\n",
        "1. **Select the Best Feature to Split**:\n",
        "\n",
        "   * The algorithm chooses a feature that best separates the data using a **criterion**:\n",
        "\n",
        "     * **Classification**: Uses **Gini Impurity**, **Entropy/Information Gain**.\n",
        "     * **Regression**: Uses **Mean Squared Error (MSE)** or **Variance Reduction**.\n",
        "\n",
        "2. **Split the Data**:\n",
        "\n",
        "   * Data is divided into subsets based on the selected feature’s values.\n",
        "\n",
        "3. **Repeat for Subsets**:\n",
        "\n",
        "   * The splitting continues **recursively** for each subset until:\n",
        "\n",
        "     * All samples in a node belong to the same class, or\n",
        "     * A maximum tree depth is reached, or\n",
        "     * There are too few samples to split further.\n",
        "\n",
        "4. **Make Predictions**:\n",
        "\n",
        "   * For a new sample, start at the root and follow the path based on the feature values until reaching a leaf node.\n",
        "\n",
        "---\n",
        "\n",
        "### **Example (Classification)**\n",
        "\n",
        "Suppose we want to predict whether someone will play tennis based on **Weather** and **Temperature**:\n",
        "\n",
        "| Weather | Temperature | Play Tennis |\n",
        "| ------- | ----------- | ----------- |\n",
        "| Sunny   | Hot         | No          |\n",
        "| Sunny   | Mild        | Yes         |\n",
        "| Rainy   | Cool        | Yes         |\n",
        "\n",
        "**Decision Tree might look like:**\n",
        "\n",
        "```\n",
        "Weather?\n",
        " ├─ Sunny → Temperature?\n",
        " │        ├─ Hot → No\n",
        " │        └─ Mild → Yes\n",
        " └─ Rainy → Yes\n",
        "```\n",
        "\n",
        "* Root Node: **Weather**\n",
        "* Internal Node: **Temperature**\n",
        "* Leaves: **Yes / No**\n",
        "\n",
        "---\n",
        "\n",
        "### **Advantages**\n",
        "\n",
        "* Easy to **interpret** and visualize.\n",
        "* Can handle both **numerical** and **categorical** data.\n",
        "* Requires little **data preprocessing** (no scaling needed).\n",
        "\n",
        "### **Disadvantages**\n",
        "\n",
        "* Prone to **overfitting** (tree may become too complex).\n",
        "* Can be **unstable**: small changes in data can lead to a different tree.\n",
        "* May be **biased** if some classes dominate.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "1SJ9m9_gm1SM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 2 What are impurity measures in Decision Trees? **\n",
        "\n",
        "Answer:\n",
        "\n",
        "In **Decision Trees**, **impurity measures** are metrics used to quantify how “mixed” or “uncertain” the samples in a node are. The main idea is:\n",
        "\n",
        "* A node is **pure** if all samples belong to the same class.\n",
        "* A node is **impure** if it contains samples from multiple classes.\n",
        "\n",
        "Decision Trees use impurity measures to decide **which feature and threshold to split on**, aiming to reduce impurity as we go down the tree.\n",
        "\n",
        "---\n",
        "\n",
        "### **Common Impurity Measures**\n",
        "\n",
        "#### 1. **Gini Impurity**\n",
        "\n",
        "* Measures the probability of **misclassifying a randomly chosen sample** from the node if it were labeled according to the class distribution in that node.\n",
        "* **Formula**:\n",
        "\n",
        "[\n",
        "Gini = 1 - \\sum_{i=1}^{C} p_i^2\n",
        "]\n",
        "\n",
        "Where:\n",
        "\n",
        "* (C) = number of classes\n",
        "\n",
        "* (p_i) = proportion of samples of class (i) in the node\n",
        "\n",
        "* **Interpretation**:\n",
        "\n",
        "  * 0 → node is pure (all samples belong to one class)\n",
        "  * Maximum → node has an equal mix of classes\n",
        "\n",
        "---\n",
        "\n",
        "#### 2. **Entropy (Information Gain)**\n",
        "\n",
        "* Measures the **amount of disorder or randomness** in the node.\n",
        "* **Formula**:\n",
        "\n",
        "[\n",
        "Entropy = - \\sum_{i=1}^{C} p_i \\log_2(p_i)\n",
        "]\n",
        "\n",
        "Where (p_i) is the proportion of samples of class (i).\n",
        "\n",
        "* **Interpretation**:\n",
        "\n",
        "  * 0 → node is pure\n",
        "  * 1 (for 2 classes) → maximum uncertainty\n",
        "\n",
        "* **Information Gain**:\n",
        "  The reduction in entropy after a split. Decision Trees choose the split that **maximizes information gain**.\n",
        "\n",
        "---\n",
        "\n",
        "#### 3. **Classification Error**\n",
        "\n",
        "* Simpler, less common measure.\n",
        "* Measures the fraction of samples that **do not belong to the majority class** in the node.\n",
        "\n",
        "[\n",
        "Error = 1 - \\max(p_i)\n",
        "]\n",
        "\n",
        "Where (p_i) is the proportion of each class.\n",
        "\n",
        "* **Interpretation**:\n",
        "\n",
        "  * 0 → node is pure\n",
        "  * Higher → node is more mixed\n",
        "\n",
        "---\n",
        "\n",
        "### **Summary Table**\n",
        "\n",
        "| Measure              | Range | Best Split Criterion      | Notes                        |\n",
        "| -------------------- | ----- | ------------------------- | ---------------------------- |\n",
        "| Gini Impurity        | 0–0.5 | Minimize Gini             | Most popular in scikit-learn |\n",
        "| Entropy              | 0–1   | Maximize Information Gain | Slightly slower to compute   |\n",
        "| Classification Error | 0–1   | Minimize error            | Rarely used in practice      |\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3D33vOO5naKU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 3  What is the mathematical formula for Gini Impurity?**\n",
        "\n",
        "Answer:\n",
        "\n",
        "The **mathematical formula for Gini Impurity** is:\n",
        "\n",
        "[\n",
        "\\text{Gini Impurity} = 1 - \\sum_{i=1}^{C} p_i^2\n",
        "]\n",
        "\n",
        "Where:\n",
        "\n",
        "* (C) = number of classes in the node\n",
        "* (p_i) = proportion of samples belonging to class (i) in that node\n",
        "\n",
        "---\n",
        "\n",
        "### **Explanation:**\n",
        "\n",
        "1. For each class (i), calculate the proportion of samples (p_i).\n",
        "2. Square each proportion ((p_i^2)) and sum them up.\n",
        "3. Subtract the sum from 1 to get the Gini Impurity.\n",
        "\n",
        "---\n",
        "\n",
        "### **Example:**\n",
        "\n",
        "Suppose a node has 10 samples:\n",
        "\n",
        "* 4 of class A\n",
        "* 6 of class B\n",
        "\n",
        "[\n",
        "p_A = 4/10 = 0.4, \\quad p_B = 6/10 = 0.6\n",
        "]\n",
        "\n",
        "[\n",
        "\\text{Gini} = 1 - (0.4^2 + 0.6^2) = 1 - (0.16 + 0.36) = 1 - 0.52 = 0.48\n",
        "]\n",
        "\n",
        "* A **Gini Impurity of 0.48** means the node is **mixed**.\n",
        "* If all samples were of one class, Gini = 0 → node is **pure**.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "LKiFpsBDn497"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 4  What is the mathematical formula for Entropy ?**\n",
        "\n",
        "Answer:\n",
        "\n",
        "The **mathematical formula for Entropy** in a decision tree node is:\n",
        "\n",
        "[\n",
        "\\text{Entropy} = - \\sum_{i=1}^{C} p_i \\log_2(p_i)\n",
        "]\n",
        "\n",
        "Where:\n",
        "\n",
        "* (C) = number of classes in the node\n",
        "* (p_i) = proportion of samples belonging to class (i) in that node\n",
        "\n",
        "---\n",
        "\n",
        "### **Explanation:**\n",
        "\n",
        "1. For each class (i), calculate the proportion (p_i).\n",
        "2. Multiply (p_i) by (\\log_2(p_i)).\n",
        "3. Sum these values for all classes.\n",
        "4. Take the negative of the sum to get the entropy.\n",
        "\n",
        "* **Entropy = 0** → node is **pure** (all samples belong to one class)\n",
        "* **Entropy is maximum** → node has **equal distribution of classes** (maximum disorder)\n",
        "\n",
        "---\n",
        "\n",
        "### **Example:**\n",
        "\n",
        "Suppose a node has 10 samples:\n",
        "\n",
        "* 4 of class A\n",
        "* 6 of class B\n",
        "\n",
        "[\n",
        "p_A = 4/10 = 0.4, \\quad p_B = 6/10 = 0.6\n",
        "]\n",
        "\n",
        "[\n",
        "\\text{Entropy} = -(0.4 \\log_2 0.4 + 0.6 \\log_2 0.6)\n",
        "]\n",
        "\n",
        "[\n",
        "\\text{Entropy} = -(0.4 \\times -1.3219 + 0.6 \\times -0.7369)\n",
        "= 0.971\n",
        "]\n",
        "\n",
        "* An **Entropy of 0.971** indicates a **mixed node**.\n",
        "* If all samples were of one class, Entropy = 0 → pure node.\n"
      ],
      "metadata": {
        "id": "Y7EGvDxZoO6D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 5  What is Information Gain, and how is it used in Decision Trees**\n",
        "\n",
        "Answer:\n",
        "\n",
        "**Information Gain (IG)** is a key concept used in **Decision Trees** to decide which feature to split on at each node. It measures **how much “information” a feature gives us about the target variable**, i.e., how much it reduces uncertainty (entropy) about the class labels.\n",
        "\n",
        "---\n",
        "\n",
        "### **Definition**\n",
        "\n",
        "Information Gain is the **reduction in entropy** after splitting a dataset based on a feature. Mathematically:\n",
        "\n",
        "[\n",
        "IG(D, A) = Entropy(D) - \\sum_{v \\in Values(A)} \\frac{|D_v|}{|D|} , Entropy(D_v)\n",
        "]\n",
        "\n",
        "Where:\n",
        "\n",
        "* (D) = dataset at the current node\n",
        "* (A) = feature being considered for splitting\n",
        "* (Values(A)) = all possible values of feature (A)\n",
        "* (D_v) = subset of (D) where feature (A) has value (v)\n",
        "* (|D_v|/|D|) = proportion of samples in that subset\n",
        "\n",
        "---\n",
        "\n",
        "### **Step-by-Step Explanation**\n",
        "\n",
        "1. **Compute Entropy of the parent node** (before split).\n",
        "2. **Split the dataset** based on a feature (A) into subsets (D_v).\n",
        "3. **Compute weighted entropy** of all subsets after the split.\n",
        "4. **Subtract the weighted entropy from the parent entropy** → gives Information Gain.\n",
        "\n",
        "* A **high IG** means the feature splits the data well (reduces uncertainty a lot).\n",
        "* The Decision Tree chooses the feature with **maximum Information Gain** at each node.\n",
        "\n",
        "---\n",
        "\n",
        "### **Example**\n",
        "\n",
        "Suppose we have 10 samples:\n",
        "\n",
        "| Class | Count |\n",
        "| ----- | ----- |\n",
        "| Yes   | 6     |\n",
        "| No    | 4     |\n",
        "\n",
        "**Step 1: Entropy of parent node**\n",
        "\n",
        "[\n",
        "Entropy(D) = - \\left( \\frac{6}{10} \\log_2 \\frac{6}{10} + \\frac{4}{10} \\log_2 \\frac{4}{10} \\right)\n",
        "= 0.971\n",
        "]\n",
        "\n",
        "**Step 2: Split by a feature (e.g., Weather)**\n",
        "\n",
        "| Weather | Yes | No | Total |\n",
        "| ------- | --- | -- | ----- |\n",
        "| Sunny   | 2   | 3  | 5     |\n",
        "| Rainy   | 4   | 1  | 5     |\n",
        "\n",
        "**Step 3: Entropy of subsets**\n",
        "\n",
        "* Sunny: (Entropy = -(2/5 \\log_2 2/5 + 3/5 \\log_2 3/5) = 0.971)\n",
        "* Rainy: (Entropy = -(4/5 \\log_2 4/5 + 1/5 \\log_2 1/5) = 0.722)\n",
        "\n",
        "**Step 4: Weighted entropy after split**\n",
        "\n",
        "[\n",
        "Entropy_{split} = \\frac{5}{10} \\cdot 0.971 + \\frac{5}{10} \\cdot 0.722 = 0.8465\n",
        "]\n",
        "\n",
        "**Step 5: Information Gain**\n",
        "\n",
        "[\n",
        "IG = Entropy(D) - Entropy_{split} = 0.971 - 0.8465 = 0.1245\n",
        "]\n",
        "\n",
        "* The tree would compare this IG with other features and pick the one with the **highest IG**.\n"
      ],
      "metadata": {
        "id": "3VcKJYs1onbQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 6  What is the difference between Gini Impurity and Entropy ?**\n",
        "\n",
        "Answer:\n",
        "\n",
        "The **difference between Gini Impurity and Entropy** lies in **how they measure impurity** in a node and the slight impact this has on how a Decision Tree chooses splits. Both are used to evaluate the “purity” of a node, but the calculation and interpretation differ.\n",
        "\n",
        "---\n",
        "\n",
        "### **1. Formula**\n",
        "\n",
        "| Measure           | Formula                                        |\n",
        "| ----------------- | ---------------------------------------------- |\n",
        "| **Gini Impurity** | ( Gini = 1 - \\sum_{i=1}^{C} p_i^2 )            |\n",
        "| **Entropy**       | ( Entropy = - \\sum_{i=1}^{C} p_i \\log_2(p_i) ) |\n",
        "\n",
        "Where (p_i) is the proportion of samples of class (i) in the node, and (C) is the number of classes.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Interpretation**\n",
        "\n",
        "* **Gini Impurity:**\n",
        "\n",
        "  * Measures the **probability of misclassifying** a randomly chosen sample from the node.\n",
        "  * Values range from **0 (pure)** to **0.5 (for 2 classes with equal distribution)**.\n",
        "\n",
        "* **Entropy:**\n",
        "\n",
        "  * Measures the **amount of disorder or uncertainty** in the node.\n",
        "  * Values range from **0 (pure)** to **1 (maximum uncertainty for 2 classes)**.\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Sensitivity**\n",
        "\n",
        "* **Entropy** uses a logarithmic scale, so it is slightly more sensitive to changes in class probabilities.\n",
        "* **Gini** is simpler and computationally faster because it avoids logarithms.\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Use in Decision Trees**\n",
        "\n",
        "* Both are used to find the **best feature to split** at a node:\n",
        "\n",
        "  * **Entropy** → split that **maximizes Information Gain**.\n",
        "  * **Gini** → split that **minimizes Gini Impurity**.\n",
        "* In practice, they often give **very similar trees**, especially with large datasets.\n",
        "* **Gini** is slightly preferred in **scikit-learn’s CART algorithm** because it’s faster to compute.\n",
        "\n",
        "---\n",
        "\n",
        "### **5. Quick Example**\n",
        "\n",
        "Suppose a node has 10 samples: 4 Yes, 6 No.\n",
        "\n",
        "* **Gini:** ( 1 - (0.4^2 + 0.6^2) = 0.48 )\n",
        "\n",
        "* **Entropy:** ( - (0.4 \\log_2 0.4 + 0.6 \\log_2 0.6) = 0.971 )\n",
        "\n",
        "* Both indicate **impurity**, but the scale is different.\n",
        "\n",
        "---\n",
        "\n",
        "✅ **Summary:**\n",
        "\n",
        "| Feature           | Gini Impurity               | Entropy            |\n",
        "| ----------------- | --------------------------- | ------------------ |\n",
        "| Formula           | 1 - Σp_i²                   | -Σp_i log₂(p_i)    |\n",
        "| Range (2 classes) | 0–0.5                       | 0–1                |\n",
        "| Sensitivity       | Less sensitive              | More sensitive     |\n",
        "| Computation       | Faster                      | Slower (logarithm) |\n",
        "| Used in           | CART (scikit-learn default) | ID3, C4.5          |\n"
      ],
      "metadata": {
        "id": "9_sEJSwCpG4t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 7  What is the mathematical explanation behind Decision Trees?**\n",
        "\n",
        "Aanswer:\n",
        "\n",
        "Here’s a detailed **mathematical explanation of Decision Trees**, covering both **classification** and **regression**.\n",
        "\n",
        "---\n",
        "\n",
        "## **1. Problem Setup**\n",
        "\n",
        "Suppose we have a dataset:\n",
        "\n",
        "[\n",
        "D = {(x_1, y_1), (x_2, y_2), \\dots, (x_n, y_n)}\n",
        "]\n",
        "\n",
        "Where:\n",
        "\n",
        "* (x_i \\in \\mathbb{R}^m) → feature vector with (m) features\n",
        "* (y_i) → target value (categorical for classification, continuous for regression)\n",
        "* (n) → number of samples\n",
        "\n",
        "The goal of a Decision Tree is to **partition the feature space** into regions (R_1, R_2, \\dots, R_J) such that the **target variable is as homogeneous as possible** within each region.\n",
        "\n",
        "---\n",
        "\n",
        "## **2. Recursive Binary Splitting**\n",
        "\n",
        "The tree is built **recursively**:\n",
        "\n",
        "1. At each node, select a feature (X_j) and a threshold (s) to split the data into two regions:\n",
        "\n",
        "[\n",
        "R_1(j, s) = { x \\mid x_j \\le s }, \\quad R_2(j, s) = { x \\mid x_j > s }\n",
        "]\n",
        "\n",
        "2. For classification: choose (X_j, s) that **maximize Information Gain** or **minimize Gini Impurity**.\n",
        "\n",
        "3. For regression: choose (X_j, s) that **minimize variance** or **mean squared error** in each region.\n",
        "\n",
        "---\n",
        "\n",
        "## **3. Impurity Measures (Classification)**\n",
        "\n",
        "### **Gini Impurity**\n",
        "\n",
        "[\n",
        "Gini(R) = 1 - \\sum_{k=1}^{K} p_k^2\n",
        "]\n",
        "\n",
        "* (p_k) = proportion of class (k) in region (R)\n",
        "* (K) = number of classes\n",
        "\n",
        "The **best split** ((j^*, s^*)) minimizes the weighted Gini:\n",
        "\n",
        "[\n",
        "(j^*, s^*) = \\arg\\min_{j,s} \\left[ \\frac{|R_1|}{|R|} Gini(R_1) + \\frac{|R_2|}{|R|} Gini(R_2) \\right]\n",
        "]\n",
        "\n",
        "---\n",
        "\n",
        "### **Entropy / Information Gain**\n",
        "\n",
        "[\n",
        "Entropy(R) = - \\sum_{k=1}^{K} p_k \\log_2(p_k)\n",
        "]\n",
        "\n",
        "* Weighted Entropy after split:\n",
        "\n",
        "[\n",
        "Entropy_{split} = \\frac{|R_1|}{|R|} Entropy(R_1) + \\frac{|R_2|}{|R|} Entropy(R_2)\n",
        "]\n",
        "\n",
        "* **Information Gain**:\n",
        "\n",
        "[\n",
        "IG = Entropy(R) - Entropy_{split}\n",
        "]\n",
        "\n",
        "* The split ((j^*, s^*)) maximizes (IG).\n",
        "\n",
        "---\n",
        "\n",
        "## **4. Splitting Criterion (Regression)**\n",
        "\n",
        "For continuous targets, the best split minimizes **variance or squared error**:\n",
        "\n",
        "[\n",
        "\\text{MSE}(R) = \\frac{1}{|R|} \\sum_{i \\in R} (y_i - \\bar{y}_R)^2\n",
        "]\n",
        "\n",
        "* (\\bar{y}*R = \\frac{1}{|R|} \\sum*{i \\in R} y_i) → mean of target in region (R)\n",
        "\n",
        "* Best split:\n",
        "\n",
        "[\n",
        "(j^*, s^*) = \\arg\\min_{j,s} \\left[ \\frac{|R_1|}{|R|} MSE(R_1) + \\frac{|R_2|}{|R|} MSE(R_2) \\right]\n",
        "]\n",
        "\n",
        "---\n",
        "\n",
        "## **5. Prediction**\n",
        "\n",
        "* **Classification:**\n",
        "  Assign the class with the **highest proportion** in the leaf:\n",
        "\n",
        "[\n",
        "\\hat{y} = \\arg\\max_k p_k\n",
        "]\n",
        "\n",
        "* **Regression:**\n",
        "  Assign the **mean value** of the target in the leaf:\n",
        "\n",
        "[\n",
        "\\hat{y} = \\bar{y}_R\n",
        "]\n",
        "\n",
        "---\n",
        "\n",
        "## **6. Stopping Criteria**\n",
        "\n",
        "The recursion stops when:\n",
        "\n",
        "* All samples in a node belong to the same class (pure)\n",
        "* Maximum tree depth is reached\n",
        "* Minimum number of samples per node is reached\n",
        "* No further split improves the impurity\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "qZDjRGG1peUr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 8  What is Pre-Pruning in Decision Trees ?**\n",
        "\n",
        "Answer:\n",
        "\n",
        "**Pre-Pruning** (also called **early stopping**) is a technique in **Decision Trees** used to **stop the tree from growing too deep** during training, in order to **prevent overfitting**.\n",
        "\n",
        "---\n",
        "\n",
        "### **1. Concept**\n",
        "\n",
        "* Decision Trees are prone to overfitting because they can keep splitting until each leaf is **pure**.\n",
        "* Pre-pruning **halts the growth of the tree early**, before it perfectly fits the training data.\n",
        "* It’s like saying: *“Stop splitting if the split isn’t significant or the node is already small enough.”*\n",
        "\n",
        "---\n",
        "\n",
        "### **2. How Pre-Pruning Works (Mathematically)**\n",
        "\n",
        "During tree construction, a node is split **only if it satisfies certain conditions**, such as:\n",
        "\n",
        "1. **Maximum depth of tree ((max_depth))**:\n",
        "   [\n",
        "   \\text{Stop splitting if depth} \\ge \\text{max_depth}\n",
        "   ]\n",
        "\n",
        "2. **Minimum samples per node ((min_samples_split))**:\n",
        "   [\n",
        "   \\text{Stop splitting if number of samples in node} < min_samples_split\n",
        "   ]\n",
        "\n",
        "3. **Minimum information gain ((min_gain))**:\n",
        "\n",
        "   * Split is performed only if the **information gain** (or Gini reduction) exceeds a threshold:\n",
        "     [\n",
        "     IG = Entropy(parent) - \\sum_v \\frac{|R_v|}{|R|} Entropy(R_v) \\ge min_gain\n",
        "     ]\n",
        "\n",
        "4. **Minimum impurity decrease ((min_impurity_decrease))**:\n",
        "\n",
        "   * Stop splitting if the decrease in impurity is too small:\n",
        "     [\n",
        "     \\Delta Gini = Gini(parent) - \\sum_v \\frac{|R_v|}{|R|} Gini(R_v) < threshold\n",
        "     ]\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Advantages of Pre-Pruning**\n",
        "\n",
        "* Prevents **overfitting**, making the tree generalize better.\n",
        "* Reduces **tree size**, improving **interpretability**.\n",
        "* Faster training because fewer splits are computed.\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Disadvantages**\n",
        "\n",
        "* Might **underfit** if the stopping criteria are too strict.\n",
        "* Requires careful tuning of parameters like `max_depth` or `min_samples_split`.\n",
        "\n",
        "---\n",
        "\n",
        "### **5. Example (Python - scikit-learn)**\n",
        "\n",
        "```python\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Pre-pruning using max depth and min samples\n",
        "tree = DecisionTreeClassifier(max_depth=3, min_samples_split=5)\n",
        "tree.fit(X_train, y_train)\n",
        "```\n",
        "\n",
        "Here:\n",
        "\n",
        "* `max_depth=3` → tree cannot grow beyond 3 levels\n",
        "* `min_samples_split=5` → nodes with fewer than 5 samples won’t be split\n"
      ],
      "metadata": {
        "id": "b35C8eRepzQa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 9  What is Post-Pruning in Decision Trees ?\n",
        "\n",
        "Answer:\n",
        "\n",
        "**Post-Pruning** (also called **cost complexity pruning** or **bottom-up pruning**) is a technique in **Decision Trees** where the tree is first **grown fully**, and then **unnecessary branches are removed** to prevent overfitting.\n",
        "\n",
        "---\n",
        "\n",
        "### **1. Concept**\n",
        "\n",
        "* Decision Trees can become very deep and **overfit the training data**.\n",
        "* Instead of stopping early (pre-pruning), post-pruning **lets the tree grow fully** and then **prunes nodes that do not contribute significantly** to predictive accuracy.\n",
        "* Think of it as **trimming a tree** after it has fully grown to remove weak branches.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. How Post-Pruning Works (Mathematically)**\n",
        "\n",
        "1. Grow the tree fully until all leaves are pure or meet minimal stopping conditions.\n",
        "\n",
        "2. Evaluate **whether removing a subtree improves generalization**. This is done using:\n",
        "\n",
        "   * **Cost Complexity Pruning**:\n",
        "\n",
        "[\n",
        "R_\\alpha(T) = R(T) + \\alpha \\cdot |T|\n",
        "]\n",
        "\n",
        "Where:\n",
        "\n",
        "* (R(T)) = misclassification rate (or MSE for regression) of tree (T)\n",
        "\n",
        "* (|T|) = number of terminal nodes (size of the tree)\n",
        "\n",
        "* (\\alpha) = complexity parameter (higher (\\alpha) → more pruning)\n",
        "\n",
        "* Subtrees are removed if pruning **reduces the total cost (R_\\alpha(T))**.\n",
        "\n",
        "3. Prune **bottom-up**: start from leaf nodes and move toward the root, removing nodes that **do not significantly reduce impurity**.\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Advantages of Post-Pruning**\n",
        "\n",
        "* Reduces **overfitting** effectively.\n",
        "* Often produces **better generalization** than pre-pruning.\n",
        "* No need to **set strict stopping rules** in advance.\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Disadvantages**\n",
        "\n",
        "* Slower than pre-pruning because the **entire tree must be grown** first.\n",
        "* Requires **validation data** or cross-validation to decide which nodes to prune.\n",
        "* Harder to interpret intermediate pruning steps.\n",
        "\n",
        "---\n",
        "\n",
        "### **5. Example (Python - scikit-learn)**\n",
        "\n",
        "```python\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Fully grow the tree\n",
        "tree = DecisionTreeClassifier(random_state=42)\n",
        "tree.fit(X_train, y_train)\n",
        "\n",
        "# Apply post-pruning using cost complexity pruning\n",
        "path = tree.cost_complexity_pruning_path(X_train, y_train)\n",
        "ccp_alphas = path.ccp_alphas\n",
        "\n",
        "# Train trees with different alphas and choose best using validation set\n",
        "trees = [DecisionTreeClassifier(random_state=42, ccp_alpha=alpha).fit(X_train, y_train) for alpha in ccp_alphas]\n",
        "```\n",
        "\n",
        "Here:\n",
        "\n",
        "* `ccp_alpha` controls how much pruning is applied.\n",
        "* Higher `ccp_alpha` → more aggressive pruning.\n",
        "\n",
        "---\n",
        "\n",
        "### **6. Pre-Pruning vs Post-Pruning**\n",
        "\n",
        "| Feature              | Pre-Pruning          | Post-Pruning                   |\n",
        "| -------------------- | -------------------- | ------------------------------ |\n",
        "| When pruning occurs  | During tree growth   | After tree is fully grown      |\n",
        "| Risk of underfitting | Higher if too strict | Lower                          |\n",
        "| Complexity           | Easier to control    | More computationally intensive |\n",
        "| Generalization       | Good if tuned        | Often better generalization    |\n"
      ],
      "metadata": {
        "id": "GQkIXJuhqKCU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 10  What is the difference between Pre-Pruning and Post-Pruning ?**\n",
        "\n",
        "Answer:\n",
        "\n",
        "Here’s a clear comparison between **Pre-Pruning** and **Post-Pruning** in Decision Trees:\n",
        "\n",
        "---\n",
        "\n",
        "### **1. Definition**\n",
        "\n",
        "| Feature        | Pre-Pruning (Early Stopping)                              | Post-Pruning (Cost Complexity Pruning)                             |\n",
        "| -------------- | --------------------------------------------------------- | ------------------------------------------------------------------ |\n",
        "| **Definition** | Stops the tree from growing **too deep during training**. | Grows the tree fully first, then **removes unnecessary branches**. |\n",
        "\n",
        "---\n",
        "\n",
        "### **2. When Pruning Happens**\n",
        "\n",
        "* **Pre-Pruning:** During tree construction.\n",
        "* **Post-Pruning:** After the tree is fully grown.\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Risk of Underfitting / Overfitting**\n",
        "\n",
        "* **Pre-Pruning:** Higher risk of **underfitting** if stopping criteria are too strict.\n",
        "* **Post-Pruning:** Lower risk of underfitting; more likely to **generalize better**.\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Stopping Criteria / Pruning Strategy**\n",
        "\n",
        "* **Pre-Pruning:** Uses thresholds like:\n",
        "\n",
        "  * Maximum depth (`max_depth`)\n",
        "  * Minimum samples per node (`min_samples_split`)\n",
        "  * Minimum information gain (`min_gain`)\n",
        "\n",
        "* **Post-Pruning:** Uses **cost complexity pruning** or validation set to remove nodes:\n",
        "\n",
        "[\n",
        "R_\\alpha(T) = R(T) + \\alpha |T|\n",
        "]\n",
        "\n",
        "* (R(T)) = error of tree (T)\n",
        "* (|T|) = number of leaves\n",
        "* (\\alpha) = complexity parameter\n",
        "\n",
        "---\n",
        "\n",
        "### **5. Advantages**\n",
        "\n",
        "| Feature        | Pre-Pruning                     | Post-Pruning                    |\n",
        "| -------------- | ------------------------------- | ------------------------------- |\n",
        "| Training speed | Faster (less computation)       | Slower (tree fully grown first) |\n",
        "| Generalization | Can underfit if criteria strict | Often better generalization     |\n",
        "| Complexity     | Easier to control               | Needs more careful validation   |\n",
        "\n",
        "---\n",
        "\n",
        "### **6. Key Idea**\n",
        "\n",
        "* **Pre-Pruning:** “Stop early” → avoids overfitting but may underfit.\n",
        "* **Post-Pruning:** “Trim after full growth” → more likely to achieve good balance between fit and generalization.\n",
        "\n"
      ],
      "metadata": {
        "id": "clVy2DMEqiid"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 11  What is a Decision Tree Regressor ?**\n",
        "\n",
        "Answer:\n",
        "\n",
        "A **Decision Tree Regressor** is a type of **Decision Tree algorithm used for regression tasks**, where the target variable is **continuous (numerical)** instead of categorical.\n",
        "\n",
        "It works similarly to a Decision Tree for classification, but instead of predicting classes, it predicts **numeric values** by partitioning the data into regions with similar target values.\n",
        "\n",
        "---\n",
        "\n",
        "### **1. How It Works**\n",
        "\n",
        "1. **Split the data recursively** based on features (X_j) and thresholds (s) to minimize **variance** (or Mean Squared Error) in the target variable (y).\n",
        "\n",
        "   For a split into regions (R_1) and (R_2):\n",
        "\n",
        "[\n",
        "\\text{MSE} = \\frac{|R_1|}{|R|} \\sum_{i \\in R_1} (y_i - \\bar{y}*{R_1})^2 + \\frac{|R_2|}{|R|} \\sum*{i \\in R_2} (y_i - \\bar{y}_{R_2})^2\n",
        "]\n",
        "\n",
        "Where:\n",
        "\n",
        "* (\\bar{y}*{R_1}), (\\bar{y}*{R_2}) = mean target in each region\n",
        "* (|R_1|, |R_2|) = number of samples in each region\n",
        "\n",
        "2. **Select the split** that **minimizes the weighted MSE** (variance) across child nodes.\n",
        "\n",
        "3. **Repeat recursively** for each child node until a stopping criterion is met (e.g., max depth, min samples per leaf).\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Prediction**\n",
        "\n",
        "* For a new sample, traverse the tree according to feature values until reaching a leaf.\n",
        "* **Predict the mean target value** of all training samples in that leaf:\n",
        "\n",
        "[\n",
        "\\hat{y} = \\frac{1}{|R|} \\sum_{i \\in R} y_i\n",
        "]\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Advantages**\n",
        "\n",
        "* Captures **non-linear relationships** in data.\n",
        "* Does **not require feature scaling**.\n",
        "* Easy to **interpret and visualize**.\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Disadvantages**\n",
        "\n",
        "* Prone to **overfitting** if the tree is too deep.\n",
        "* Can be **unstable**: small changes in data can lead to different trees.\n",
        "* Predicts **piecewise constant values**, so the prediction function is not smooth.\n",
        "\n",
        "---\n",
        "\n",
        "### **5. Example (Python - scikit-learn)**\n",
        "\n",
        "```python\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "# Create and train regressor\n",
        "regressor = DecisionTreeRegressor(max_depth=3, random_state=42)\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = regressor.predict(X_test)\n",
        "```\n",
        "\n",
        "* `max_depth=3` → prevents overfitting by limiting tree growth.\n",
        "* `y_pred` → numeric predictions for test data.\n",
        "\n"
      ],
      "metadata": {
        "id": "5oys8ssjqifj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 12  What are the advantages and disadvantages of Decision Trees?**\n",
        "\n",
        "Answer:\n",
        "\n",
        "Here’s a detailed overview of the **advantages and disadvantages of Decision Trees**:\n",
        "\n",
        "---\n",
        "\n",
        "## **Advantages of Decision Trees**\n",
        "\n",
        "1. **Easy to Understand and Interpret**\n",
        "\n",
        "   * Decision Trees mimic human decision-making and can be visualized as flowcharts.\n",
        "   * Even non-technical users can understand the rules.\n",
        "\n",
        "2. **Handles Both Numerical and Categorical Data**\n",
        "\n",
        "   * Works with continuous and categorical features without requiring extensive preprocessing.\n",
        "\n",
        "3. **No Need for Feature Scaling**\n",
        "\n",
        "   * Unlike algorithms like SVM or KNN, Decision Trees **don’t require normalization or standardization**.\n",
        "\n",
        "4. **Captures Non-Linear Relationships**\n",
        "\n",
        "   * Can model complex patterns that linear models cannot.\n",
        "\n",
        "5. **Automatic Feature Selection**\n",
        "\n",
        "   * Splits are based on the most informative features (high information gain or Gini reduction).\n",
        "\n",
        "6. **Can Handle Missing Values** (in some implementations)\n",
        "\n",
        "   * Some tree algorithms can handle missing data by surrogate splits or probabilistic assignment.\n",
        "\n",
        "7. **Versatile**\n",
        "\n",
        "   * Can be used for **classification** (Decision Tree Classifier) or **regression** (Decision Tree Regressor).\n",
        "\n",
        "---\n",
        "\n",
        "## **Disadvantages of Decision Trees**\n",
        "\n",
        "1. **Prone to Overfitting**\n",
        "\n",
        "   * Trees can become very deep and fit noise in the training data.\n",
        "   * Pre-pruning or post-pruning is often needed to prevent this.\n",
        "\n",
        "2. **Unstable / Sensitive to Data Variations**\n",
        "\n",
        "   * Small changes in the dataset can lead to a completely different tree structure.\n",
        "\n",
        "3. **Greedy Algorithm**\n",
        "\n",
        "   * Uses a **locally optimal split** at each node (maximizing information gain or Gini reduction) and may not find the global optimal tree.\n",
        "\n",
        "4. **Bias Toward Features with More Levels**\n",
        "\n",
        "   * Features with more categories can appear more informative than they really are.\n",
        "\n",
        "5. **Not Good for Extrapolation (Regression)**\n",
        "\n",
        "   * Predicts piecewise constant values, so it cannot extrapolate beyond the range of training data.\n",
        "\n",
        "6. **Can Become Complex**\n",
        "\n",
        "   * Very deep trees can be hard to interpret and may require pruning to simplify.\n",
        "\n",
        "---\n",
        "\n",
        "### **Summary Table**\n",
        "\n",
        "| Aspect             | Pros                                   | Cons                                           |\n",
        "| ------------------ | -------------------------------------- | ---------------------------------------------- |\n",
        "| Interpretability   | Very easy to visualize and explain     | Deep trees can become complex                  |\n",
        "| Data type handling | Handles numeric & categorical features | Biased toward features with many categories    |\n",
        "| Preprocessing      | No scaling needed                      | Sensitive to noise                             |\n",
        "| Accuracy           | Captures non-linear relationships      | Prone to overfitting                           |\n",
        "| Stability          | Simple rules                           | Small data changes can produce different trees |\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "SSGXxRPhrRPC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 13  How does a Decision Tree handle missing values ?**\n",
        "\n",
        "Answer:\n",
        "\n",
        "Decision Trees can handle **missing values** in several ways, depending on the algorithm and implementation. Unlike many other algorithms that require complete data, Decision Trees can **still make splits even if some feature values are missing**.\n",
        "\n",
        "Here’s how it works:\n",
        "\n",
        "---\n",
        "\n",
        "### **1. Surrogate Splits (CART method)**\n",
        "\n",
        "* When the primary splitting feature is **missing for a sample**, the tree uses a **surrogate feature** that closely mimics the primary split.\n",
        "* Surrogate splits are **learned during training** by finding features that produce similar splits to the primary feature.\n",
        "* The sample is then routed according to the surrogate feature.\n",
        "\n",
        "**Example:**\n",
        "\n",
        "* Primary split: `Age > 30`\n",
        "* If `Age` is missing, use `Experience > 5` as surrogate if it closely correlates with `Age`.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Probabilistic / Fractional Assignment**\n",
        "\n",
        "* Some implementations assign a **sample with missing value to both branches** but with **weights proportional to the number of samples** that go each way.\n",
        "* Prediction is then calculated as a **weighted average** (for regression) or **weighted vote** (for classification).\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Imputation Before Splitting**\n",
        "\n",
        "* Another approach is to **fill in missing values** before training using:\n",
        "\n",
        "  * Mean/median (for numeric features)\n",
        "  * Mode (for categorical features)\n",
        "  * More advanced imputation like k-NN or iterative imputation.\n",
        "\n",
        "* While not intrinsic to trees, this ensures splits can be computed normally.\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Ignore Missing Values in Split Calculation**\n",
        "\n",
        "* Some algorithms simply **exclude samples with missing values** when computing the best split for that feature.\n",
        "* Samples are routed later using other features.\n",
        "\n",
        "---\n",
        "\n",
        "### **Summary Table**\n",
        "\n",
        "| Method                     | How It Works                                 | Pros                     | Cons                             |\n",
        "| -------------------------- | -------------------------------------------- | ------------------------ | -------------------------------- |\n",
        "| Surrogate Splits           | Use alternate features to decide split       | Preserves tree structure | More complex to compute          |\n",
        "| Probabilistic Assignment   | Split sample into both branches with weights | Uses all data            | Slightly more computation        |\n",
        "| Imputation Before Training | Fill missing values before building tree     | Simple to implement      | Depends on quality of imputation |\n",
        "| Ignore Missing Values      | Skip missing samples when computing splits   | Easy                     | May lose information             |\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "O2WacFUMrliU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 14  How does a Decision Tree handle categorical features ?**\n",
        "\n",
        "Answer:\n",
        "\n",
        "Decision Trees can naturally handle **categorical features** without needing one-hot encoding or scaling. They do this by **splitting the data based on the categories** in a way that maximizes homogeneity (purity) in the resulting nodes.\n",
        "\n",
        "Here’s how it works:\n",
        "\n",
        "---\n",
        "\n",
        "### **1. Splitting on Categorical Features**\n",
        "\n",
        "#### **a) Binary Splits**\n",
        "\n",
        "* For a categorical feature with multiple categories, the tree can create **binary splits** by dividing the categories into two groups.\n",
        "* The split that **maximizes information gain** (or minimizes Gini impurity) is chosen.\n",
        "\n",
        "**Example:**\n",
        "Feature: `Color = {Red, Blue, Green}`\n",
        "\n",
        "* Possible binary splits:\n",
        "\n",
        "  1. `{Red} vs {Blue, Green}`\n",
        "  2. `{Blue} vs {Red, Green}`\n",
        "  3. `{Green} vs {Red, Blue}`\n",
        "\n",
        "* The split with the **lowest impurity** in child nodes is selected.\n",
        "\n",
        "---\n",
        "\n",
        "#### **b) Multi-way Splits (Some Implementations)**\n",
        "\n",
        "* Some tree algorithms (like **ID3**) allow **one branch per category**.\n",
        "* Each category gets its own child node.\n",
        "\n",
        "**Example:**\n",
        "Feature: `Day = {Mon, Tue, Wed}` → three branches: Mon, Tue, Wed\n",
        "\n",
        "* This can lead to **larger trees** and is less common in CART-based trees (scikit-learn).\n",
        "\n",
        "---\n",
        "\n",
        "### **2. How the Algorithm Decides the Split**\n",
        "\n",
        "* Calculate **impurity measures** (Gini, Entropy, or classification error) for all possible splits of the categorical feature.\n",
        "* Select the split that **reduces impurity the most**.\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Advantages**\n",
        "\n",
        "* No need for encoding like one-hot or label encoding (though some implementations may require label encoding).\n",
        "* Can handle **high-cardinality categorical features** by grouping categories.\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Example**\n",
        "\n",
        "Suppose we have:\n",
        "\n",
        "| Weather  | Play Tennis |\n",
        "| -------- | ----------- |\n",
        "| Sunny    | No          |\n",
        "| Rainy    | Yes         |\n",
        "| Overcast | Yes         |\n",
        "| Sunny    | Yes         |\n",
        "\n",
        "**Step 1:** Consider categorical feature `Weather = {Sunny, Rainy, Overcast}`\n",
        "**Step 2:** Evaluate splits like:\n",
        "\n",
        "* `{Sunny} vs {Rainy, Overcast}`\n",
        "* `{Rainy} vs {Sunny, Overcast}`\n",
        "* `{Overcast} vs {Sunny, Rainy}`\n",
        "\n",
        "**Step 3:** Choose split that **maximizes information gain** (Entropy reduction) or **minimizes Gini impurity**.\n",
        "\n",
        "---\n",
        "\n",
        "✅ **Summary:**\n",
        "\n",
        "* Decision Trees handle categorical features by **splitting based on category membership**, either with **binary grouping** or **multi-way splits**, and select the split that maximizes node purity.\n",
        "* This allows trees to work naturally with categorical data without extensive preprocessing.\n"
      ],
      "metadata": {
        "id": "Zy2wUyLAr6h4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 15  What are some real-world applications of Decision Trees?**\n",
        "\n",
        "Answer:\n",
        "\n",
        "**Decision Trees** are widely used in real-world applications because they are **interpretable, versatile, and can handle both classification and regression tasks**. Here are some notable applications:\n",
        "\n",
        "---\n",
        "\n",
        "### **1. Healthcare and Medical Diagnosis**\n",
        "\n",
        "* **Disease prediction**: Predicting whether a patient has a disease based on symptoms and test results.\n",
        "* **Risk assessment**: Determining patient risk levels for heart disease, diabetes, or cancer.\n",
        "* Example: Using patient age, blood pressure, cholesterol to classify heart disease risk.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Finance and Banking**\n",
        "\n",
        "* **Credit scoring**: Deciding whether to approve a loan or credit card based on customer financial history.\n",
        "* **Fraud detection**: Identifying fraudulent transactions based on transaction patterns.\n",
        "* Example: Classifying a credit card transaction as “fraudulent” or “legitimate.”\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Marketing and Customer Analytics**\n",
        "\n",
        "* **Customer segmentation**: Grouping customers based on purchasing behavior or demographics.\n",
        "* **Churn prediction**: Predicting whether a customer is likely to leave a service.\n",
        "* Example: Telecom companies predicting churn using features like call usage, plan type, and complaints.\n",
        "\n",
        "---\n",
        "\n",
        "### **4. E-commerce and Retail**\n",
        "\n",
        "* **Recommendation systems**: Suggesting products based on customer preferences and previous purchases.\n",
        "* **Sales prediction**: Predicting demand or revenue based on historical data.\n",
        "\n",
        "---\n",
        "\n",
        "### **5. Manufacturing and Industry**\n",
        "\n",
        "* **Quality control**: Detecting defective products based on manufacturing parameters.\n",
        "* **Predictive maintenance**: Predicting machine failures using sensor data.\n",
        "\n",
        "---\n",
        "\n",
        "### **6. Agriculture**\n",
        "\n",
        "* **Crop prediction**: Predicting crop yield based on soil, weather, and irrigation data.\n",
        "* **Disease detection in plants**: Classifying plant diseases from images or environmental conditions.\n",
        "\n",
        "---\n",
        "\n",
        "### **7. Human Resources**\n",
        "\n",
        "* **Employee attrition**: Predicting which employees are likely to leave based on performance and engagement metrics.\n",
        "* **Recruitment screening**: Classifying resumes or candidates based on experience, skills, and other features.\n",
        "\n",
        "---\n",
        "\n",
        "### **8. Real Estate**\n",
        "\n",
        "* **Property price prediction**: Regression trees predict house prices based on location, size, and amenities.\n",
        "* **Investment decisions**: Classifying properties as high or low investment potential.\n",
        "\n",
        "---\n",
        "\n",
        "### **9. Environment and Climate**\n",
        "\n",
        "* **Weather prediction**: Classifying types of weather (rain, snow, sunny) based on atmospheric data.\n",
        "* **Pollution detection**: Predicting air quality levels in different regions.\n",
        "\n",
        "---\n",
        "\n",
        "### **Why Decision Trees Are Popular in These Applications**\n",
        "\n",
        "* **Interpretability**: Easy to explain decisions to non-technical stakeholders.\n",
        "* **Handles different data types**: Works with numeric and categorical features.\n",
        "* **No preprocessing required**: Can work without scaling or normalization.\n"
      ],
      "metadata": {
        "id": "Qf_1hJorsNd9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PRACTICAL QUESION**"
      ],
      "metadata": {
        "id": "FkQ0Yjmoshva"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 16 Write a Python program to train a Decision Tree Classifier on the Iris dataset and print the model accuracy**\n"
      ],
      "metadata": {
        "id": "ZEkwgL40sm61"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZI6rC9OmSl1",
        "outputId": "8d3ca6ac-ff22-4e54-f9aa-2beea3ef4b76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Classifier Accuracy: 1.00\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data       # Features\n",
        "y = iris.target     # Target classes\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create the Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Train the classifier\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate and print the model accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Decision Tree Classifier Accuracy: {accuracy:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 17  Write a Python program to train a Decision Tree Classifier using Gini Impurity as the criterion and print the feature importances**"
      ],
      "metadata": {
        "id": "37fjsLk7tJ6k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data       # Features\n",
        "y = iris.target     # Target classes\n",
        "feature_names = iris.feature_names\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create the Decision Tree Classifier using Gini Impurity\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "\n",
        "# Train the classifier\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Print feature importances\n",
        "print(\"Feature Importances:\")\n",
        "for name, importance in zip(feature_names, clf.feature_importances_):\n",
        "    print(f\"{name}: {importance:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A6W_Tt_ns6zg",
        "outputId": "3648eca7-5a12-4d0b-c740-0e1918300e6d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importances:\n",
            "sepal length (cm): 0.0000\n",
            "sepal width (cm): 0.0167\n",
            "petal length (cm): 0.9061\n",
            "petal width (cm): 0.0772\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 18  Write a Python program to train a Decision Tree Classifier using Entropy as the splitting criterion and print the model accuracy**"
      ],
      "metadata": {
        "id": "lcOfARPutlhA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data       # Features\n",
        "y = iris.target     # Target classes\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create the Decision Tree Classifier using Entropy\n",
        "clf = DecisionTreeClassifier(criterion='entropy', random_state=42)\n",
        "\n",
        "# Train the classifier\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate and print the model accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Decision Tree Classifier Accuracy (Entropy): {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-5ibupTgtizd",
        "outputId": "d29d5fe1-418a-444e-bbc3-0241f69330f6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Classifier Accuracy (Entropy): 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 20  Write a Python program to train a Decision Tree Classifier and visualize the tree using graphviz?**"
      ],
      "metadata": {
        "id": "pkjwH5fdukhs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
        "import graphviz\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "feature_names = iris.feature_names\n",
        "class_names = iris.target_names\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create the Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier(criterion='gini', max_depth=3, random_state=42)\n",
        "\n",
        "# Train the classifier\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Export the tree to Graphviz format\n",
        "dot_data = export_graphviz(\n",
        "    clf,\n",
        "    out_file=None,\n",
        "    feature_names=feature_names,\n",
        "    class_names=class_names,\n",
        "    filled=True,\n",
        "    rounded=True,\n",
        "    special_characters=True\n",
        ")\n",
        "\n",
        "# Visualize the tree\n",
        "graph = graphviz.Source(dot_data)\n",
        "graph.render(\"decision_tree_iris\", format=\"png\", cleanup=True)  # Saves as PNG\n",
        "graph.view()  # Opens the image\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "2mFwpfrWugv4",
        "outputId": "bb3b3b43-01ab-44a8-cc91-977571d88f21"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'decision_tree_iris.pdf'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 21  Write a Python program to train a Decision Tree Classifier with a maximum depth of 3 and compare its accuracy with a fully grown tree**"
      ],
      "metadata": {
        "id": "mmwvECeAu6hn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Decision Tree Classifier with max depth = 3 (pre-pruned)\n",
        "clf_pruned = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "clf_pruned.fit(X_train, y_train)\n",
        "y_pred_pruned = clf_pruned.predict(X_test)\n",
        "accuracy_pruned = accuracy_score(y_test, y_pred_pruned)\n",
        "\n",
        "# Train a fully grown Decision Tree Classifier (no depth limit)\n",
        "clf_full = DecisionTreeClassifier(random_state=42)\n",
        "clf_full.fit(X_train, y_train)\n",
        "y_pred_full = clf_full.predict(X_test)\n",
        "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Accuracy of Decision Tree with max_depth=3: {accuracy_pruned:.2f}\")\n",
        "print(f\"Accuracy of Fully Grown Decision Tree: {accuracy_full:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2OA_zsh2uyMN",
        "outputId": "71ad051c-5634-4a0f-afc1-266035e06551"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Decision Tree with max_depth=3: 1.00\n",
            "Accuracy of Fully Grown Decision Tree: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 22 Write a Python program to train a Decision Tree Classifier using min_samples_split=5 and compare its accuracy with a default tree**"
      ],
      "metadata": {
        "id": "yD1H86EWvNs_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Decision Tree with min_samples_split=5 (pre-pruning)\n",
        "clf_min_samples = DecisionTreeClassifier(min_samples_split=5, random_state=42)\n",
        "clf_min_samples.fit(X_train, y_train)\n",
        "y_pred_min_samples = clf_min_samples.predict(X_test)\n",
        "accuracy_min_samples = accuracy_score(y_test, y_pred_min_samples)\n",
        "\n",
        "# Train a default fully grown Decision Tree\n",
        "clf_default = DecisionTreeClassifier(random_state=42)\n",
        "clf_default.fit(X_train, y_train)\n",
        "y_pred_default = clf_default.predict(X_test)\n",
        "accuracy_default = accuracy_score(y_test, y_pred_default)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Accuracy of Decision Tree with min_samples_split=5: {accuracy_min_samples:.2f}\")\n",
        "print(f\"Accuracy of Default Fully Grown Tree: {accuracy_default:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GFyWqU01vFys",
        "outputId": "5799cab6-659c-42c1-a7d8-02fd46c64e85"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Decision Tree with min_samples_split=5: 1.00\n",
            "Accuracy of Default Fully Grown Tree: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 23  Write a Python program to apply feature scaling before training a Decision Tree Classifier and compare its accuracy with unscaled data**"
      ],
      "metadata": {
        "id": "lCYKICRevcDo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# -----------------------------\n",
        "# 1. Train Decision Tree on unscaled data\n",
        "# -----------------------------\n",
        "clf_unscaled = DecisionTreeClassifier(random_state=42)\n",
        "clf_unscaled.fit(X_train, y_train)\n",
        "y_pred_unscaled = clf_unscaled.predict(X_test)\n",
        "accuracy_unscaled = accuracy_score(y_test, y_pred_unscaled)\n",
        "\n",
        "# -----------------------------\n",
        "# 2. Apply feature scaling\n",
        "# -----------------------------\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train Decision Tree on scaled data\n",
        "clf_scaled = DecisionTreeClassifier(random_state=42)\n",
        "clf_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = clf_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "# -----------------------------\n",
        "# Print results\n",
        "# -----------------------------\n",
        "print(f\"Accuracy without feature scaling: {accuracy_unscaled:.2f}\")\n",
        "print(f\"Accuracy with feature scaling: {accuracy_scaled:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88mxAYHDva9b",
        "outputId": "b5d3e91d-4b73-403a-e163-99edf5846101"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without feature scaling: 1.00\n",
            "Accuracy with feature scaling: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 24  Write a Python program to train a Decision Tree Classifier using One-vs-Rest (OvR) strategy for multiclass classification**"
      ],
      "metadata": {
        "id": "8KVaZhjfvuLz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create the One-vs-Rest (OvR) classifier with Decision Tree as base estimator\n",
        "ovr_clf = OneVsRestClassifier(DecisionTreeClassifier(random_state=42))\n",
        "\n",
        "# Train the OvR classifier\n",
        "ovr_clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = ovr_clf.predict(X_test)\n",
        "\n",
        "# Calculate and print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of Decision Tree with OvR strategy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nEZ-kEX7vs_z",
        "outputId": "67081d82-4dda-4e96-cf62-102e35977c16"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Decision Tree with OvR strategy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 25  Write a Python program to train a Decision Tree Classifier and display the feature importance scores**"
      ],
      "metadata": {
        "id": "3217dPgiv9h5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "feature_names = iris.feature_names\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Display feature importance scores\n",
        "print(\"Feature Importances:\")\n",
        "for feature, importance in zip(feature_names, clf.feature_importances_):\n",
        "    print(f\"{feature}: {importance:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q5hufFpFv8Wy",
        "outputId": "90bc41de-ec83-4ade-e53a-1528a6b9c4a7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importances:\n",
            "sepal length (cm): 0.0000\n",
            "sepal width (cm): 0.0167\n",
            "petal length (cm): 0.9061\n",
            "petal width (cm): 0.0772\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qdkklVyzwa-i"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}